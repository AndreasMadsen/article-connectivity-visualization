<!doctype html>
<meta charset="utf-8">

<script src="https://distill.pub/template.v1.js"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">

<script defer src="bundle.js"></script>
<link href="style.css" rel="stylesheet">

<script type="text/front-matter">
  title: "Visualizing memorization in RNNs"
  description: >
    When do Recurrent Neural Networks use short-term memorization
    or long-term memorization, and how do different architectures compare
    in terms of this? Visualizing the gradient can be a powerful tool to
    answer these questions.
  authors:
  - Andreas Madsen: https://github.com/AndreasMadsen
  affiliations:
  - nearForm: https://nearform.com
</script>

<dt-article>
  <h1>Visualizing memorization in RNNs</h1>
  <h2>When do Recurrent Neural Networks use short-term memorization
  or long-term memorization, and how do different architectures compare
  in terms of this? Visualizing the gradient can be a powerful tool to
  answer these questions.
  </h2>
  <figure id="ar-hero">
    <div id="ar-hero-diagram-nlstm" class="ar-hero"></div>
    <div id="ar-hero-diagram-lstm" class="ar-hero"></div>
    <div id="ar-hero-diagram-gru" class="ar-hero"></div>
    <figcaption><strong>Connectivity visualization:</strong> visualizes
    the connectivity between the desired target and the input charecters,
    for the <a href="#ar-section-autocomplete">autocomplete problem</a>. Notice
    for example in the prediction of "gramma", how the RNN
    <a href="javascript:heroSetIndex(28);">initially</a> uses long-term
    memorization but as <a href="javascript:heroSetIndex(34);">more charecters
    become available</a> the RNN switches to short-term memorization
    (<a href="javascript:heroReset();">reset</a>).
    </figcaption>
  </figure>
  <dt-byline></dt-byline>
</dt-article>

<dt-article>
  <h2>Introduction</h2>
  <p>
    Memorization in Recurrent Neural Networks continues to pose a challenge
    in many applications. The Long-Short-Term Memory (LSTM) unit and Gated
    Recurrent Unit (GRU) are typically used to solve this problem, by
    solving the vanishing gradient problem. However, the practical problem
    of memorization still poses a challenge.
  </p>
  <p>
    Recently a new paper by Monzi et al. <dt-cite key="moniz2018nlstm"></dt-cite>
    have proposed an extension of the LSTM unit, called Nested LSTM. In that
    paper, they visualize an individual cell activation to show that the
    Nested LSTM unit has a superior ability to memorize.
  </p>
  <p>
    This visualization is inspired by another paper
    <dt-cite key="karpathy2015rnnvis"></dt-cite> where they identify cells
    that captures a specific feature. For the purpose of identifying a specific
    feature, this visualization approach works well. However, it is not a good
    argument for memorization in general as the output is entirely dependent
    on what feature the specific cell captures.
  </p>
  <p>
    This article proposes a better visualization method of connectivity for
    showing memorization and contextual understanding. Furthermore, the
    autocomplete problem is used as the main problem, as the output is much
    easier to reason about compared to more standard problems such as
    Penn Treebank, Chinese Poetry Generation, or text8 generation.
  </p>
  <p>
    It is the hope that the connectivity visualization used on the
    autocomplete problem, will help advance the dialogue around contextual
    understanding and memorization in recurrent units.
  </p>
</dt-article>

<dt-article>
  <h2>Recurrent Units</h2>
  <p>
    To express the different recurrent units as variations of the same RNN, the
    following <math-latex latex="\mathrm{Unit}(\cdot, \cdot)"></math-latex>
    notation is used:
    <math-latex display-mode latex="\begin{aligned}
      h_{\ell}^{t} &= \mathrm{Unit}(h_{\ell-1}^{t}, h_{\ell}^{t-1}), \text{ where: } h_{0}^t = x_t \\
      y^t &= \mathrm{Softmax}(h_L^t) \\
      \mathcal{L} &= - \frac{1}{T} \sum_{t=1}^T \ln(y^t_k)
    \end{aligned}"></math-latex>

    For a vanilla recurrent neural network, the recurrent unit
    <math-latex latex="\mathrm{Unit}(\cdot, \cdot)"></math-latex>
    is:
    <math-latex display-mode latex="
      h_{\ell}^{t} = \mathrm{RNN}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \coloneqq \sigma(W_{\ell-1,\ell}h_{\ell-1}^t + W_{\ell,\ell}h_{\ell}^{t-1})
    "></math-latex>
  </p>
  <p>
    In theory, the time dependency allows it in each iteration to know
    about every part of the sequence that came before. However, because
    of this time dependency it is very common for the network to suffer from a
    vanishing gradient problem, that causes the training to ignore long-term
    dependencies.
  </p>
  <figure id="ar-memorization-problem-rnn">
    <figcaption>
      <strong>Vanishing Gradient: </strong> where the contribution from the
      earlier steps becomes insignificant in the gradient:
      <math-latex latex="
      \frac{\partial \mathcal{L}}{\partial h_\ell^t} = \delta_\ell^t = \sigma(z_\ell^t) \odot \left( W_{\ell, \ell+1} \delta_{\ell+1}^t +  W_{\ell, \ell} \delta_{\ell}^{t+1}\right)
      "></math-latex>
    </figcaption>
  </figure>
  <p>
    Several solutions to the vanishing gradient problem have been made over
    the years. Most popular is the Long Short-Term Memory (LSTM)
    <dt-cite key="graves2008phd"></dt-cite> unit and the
    Gated Recurrent Unit (GRU)<dt-cite key="cho2014gru"></dt-cite>.
    Recently, the Nested LSTM unit has also been proposed
    <dt-cite key="moniz2018nlstm"></dt-cite>. Both LSTM and GRU are well known
    and <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">
    thoroughly explained in the literature</a>. An explanation on Nested LSTM
    can be found <a href="#appendix-nestedlstm">in the appendix</a>.
  </p>
  <figure id="ar-recurrent-units" data-show="nlstm">
    <img class="ar-recurrent-unit-nlstm" src="graphics/nlstm-web.svg" alt="Nested LSTM Diagram">
    <figcaption class="ar-recurrent-unit-nlstm">
      <strong>Recurrent Unit, Nested LSTM:</strong> makes the cell update depend on another
      LSTM unit, supposedly this allows more long-term memory compared to
      stacking LSTM layers.
    </figcaption>
    <img class="ar-recurrent-unit-lstm" src="graphics/lstm-web.svg" alt="Long Short Term Memory Diagram">
    <figcaption class="ar-recurrent-unit-lstm">
      <strong>Recurrent Unit, LSTM:</strong> allows for long-term
      memorization by gateing its update, thereby solving the vanishing gradient
      problem.
    </figcaption>
    <img class="ar-recurrent-unit-gru" src="graphics/gru-web.svg" alt="Gated Recurrent Unit Diagram">
    <figcaption class="ar-recurrent-unit-gru">
      <strong>Recurrent Unit, GRU:</strong> solves the vanishing gradient
        problem without depending on an internal memory state.
    </figcaption>
  </figure>
  <p>
    It is not entirely clear why one recurrent unit is better than another
    in some applications, while in other applications it is another type of
    recurrent unit that is better. Theoretical they all solve the vanishing
    gradient problem but in practice, their performance is very application
    dependent.
  </p>
  <p>
    Understanding why these differences occur is likely a very difficult
    problem. The purpose of this article is to demonstrate a visualization
    technique that can better highlight what these differences are. Hopefully,
    such understanding can lead to a deeper understanding in the future.
  </p>
</dt-article>

<dt-article>
  <h2>Comparing Recurrent Units</h2>
  <p>
    Comparing the different Recurrent Units is an important task and it is
    often much more complex than just comparing the accuracy or cross entropy
    loss. This is because differences in these high-level quantitative measures
    can have many explanations and may simply come down to some small improvement
    in predictions that only requires short-term memorization. While in reality,
    it is often the long-term memorization that is of interest to us.
  </p>
  <p>
    A good problem for qualitatively analyzing the contextual understanding,
    should therefore have a humanly interpretable output and depend both on
    long and short memorization.
  </p>
  <p id="ar-section-autocomplete">
    To this end, the autocomplete problem is used. Each character is mapped
    to a target that represents the entire word. To make it extra difficult,
    the space leading up to the word should also map to that word. The
    prediction for the space charecter, is in particular useful for showing
    contextual understanding.
  </p>
  <figure id="ar-demo">
    <figcaption id="ar-demo-input-caption">User types input sequence.</figcaption>
    <div id="ar-demo-input-content"></div>

    <figcaption id="ar-demo-rnn-caption">Recurrent neural network processes the sequence.</figcaption>
    <div id="ar-demo-rnn-content"></div>

    <figcaption id="ar-demo-output-caption">The output for the last character is used.</figcaption>
    <div id="ar-demo-output-content"></div>

    <figcaption id="ar-demo-final-caption">The most likely suggestions are extracted.</figcaption>
    <div id="ar-demo-final-content"></div>

    <figcaption id="ar-demo-caption">
      <strong>Autocomplete:</strong> An application that has a humanly
      interpretable output, while depending on both short and long term
      contextual understanding. In this case, the network uses past information
      and understands the next word should be a country. Try
      <a href="javascript:arDemoShort();">removing the last letters</a> to see
      that the network continues to give meaningful suggestions
      (<a href="javascript:arDemoReset();">reset</a>).
      <span style="font-style: italic" id="ar-demo-input-notice"></span>
    </figcaption>
  </figure>
  <p>
    The autocomplete dataset is constructed from the full
    <a href="http://mattmahoney.net/dc/textdata.html">text8</a> dataset. The
    recurrent neural networks used to solve the problem have two layers, each
    with 600 units. There are 3 models, using GRU, LSTM, and Nested LSTM.
    See <a href="#appendix-autocomplete">the appendix</a> for more details.
  </p>
  <figure class="ar-table-container">
    <table class="ar-table">
      <thead>
        <tr><th>Model</th><th>Cross Entropy</th><th>Accuracy</th></tr>
      </thead>
      <tbody>
        <tr><td>GRU</td><td>2.1497</td><td>51.61%</td>
        <tr><td>LSTM</td><td>2.2899</td><td>49.90%</td>
        <tr><td>Nested LSTM</td><td>2.6051</td><td>45.47%</td>
      </tbody>
    </table>
    <figcaption>
      <strong>Model testing:</strong> shows the testing loss and accuracy
      for the GRU, LSTM, and Nested LSTM models on the autocomplete problem.
    </figcaption>
  </figure>
  <h3>Connectivity in the Autocomplete Problem</h3>
  <p>
  To get a better idea of how well each model memorizes and uses memory for
  contextual understanding, the connectivity between the desired output and
  the input should be analyzed. This is calculated as the magnitude of
  the gradient, between the logits for the desired output
  <math-latex latex="(h_L^{\tilde{t}})_k"></math-latex> and the input
  <math-latex latex="x^t"></math-latex>.
  <math-latex display-mode latex="
    \textrm{connectivity}(t, \tilde{t}) =
    \left|\left|\frac{\partial (h_L^{\tilde{t}})_k}{\partial x^t}\right|\right|_2
  "></math-latex>
  </p>
  <figure>
    <div id="ar-connectivity-nlstm" class="ar-connectivity"></div>
    <div id="ar-connectivity-lstm" class="ar-connectivity"></div>
    <div id="ar-connectivity-gru" class="ar-connectivity"></div>
    <figcaption>
      <strong>Connectivity:</strong> shows the connection strength between
      the target for the selected character and the input characters
      (<a href="javascript:connectivitySetIndex(null);">reset</a>).
    </figcaption>
  </figure>
  <p>
    Exploring the connectivity gives a surprising amount of insight into the
    different models ability for long-term contextual understanding. You should
    try and interact with the figure yourself, to see what information the
    different models use.
  </p>
  <p>
    Here are two interesting observations:
  </p>
  <ul>
    <li>
  The first observation, is when the <a href="javascript:connectivitySetIndex(105);">
  models should predict "learning" and is only given data
  until the first character</a>. Here it is clear, that the Nested LSTM
  model uses no past information and thus only suggests common words starting
  with the letter "l".<br>

  The LSTM model suggests variations on the word "language",
  which makes sense given the text is about learning grammar. The GRU model
  does one better, by using the words "study" and "education" to also suggests
  the correct word "learning".
    </li>
    <li>
  The second observation, is when the models should predict the word "grammar".
  This word appears twice, for the first case very little is known
  about the context. Thus, the models don't suggest "grammar" until they have
  <a href="javascript:connectivitySetIndex(31);">seen 3</a> or
  <a href="javascript:connectivitySetIndex(32);">4 characters</a>.<br>

  For the second occurrence, there is much more text for the models to
  understand the context of the word. The GRU model is thus able to predict
  the word "grammar" with only <a href="javascript:connectivitySetIndex(159);">
  1 character from the word itself</a>. While the LSTM and Nested LSTM again
  needs <a href="javascript:connectivitySetIndex(162);">4 characters</a>.
    </li>
  </ul>
  <p>
    These observations suggest that the Nested LSTM model, in particular,
    doesn't use long-term memorization as one would otherwise expect from the
    additional memory states.
  </p>
</dt-article>

<dt-article>
  <h2>Conclusion</h2>
  <p>
    Looking at accuracy and cross entropy loss in itself is not that
    interesting. A reasonable high accuracy can be obtained, with reasonably
    low contextual understanding. It is only for the first couple of characters,
    that long-term memorization and contextual understanding really matters.
  </p>
  <p>
    A qualitative analysis, where one looks at how previous input is used in
    the prediction is therefore also important when judging models. In this
    case, the connectivity visualization together with the autocomplete
    predictions, reveals that the Nested LSTM model isn’t capable of long-term
    contextual understanding. This contradicts the original design hypothesis
    and findings for the Nested LSTM model, that more internal memory leads to
    more long-term memorization <dt-cite key="moniz2018nlstm"></dt-cite>.
    In fact, the GRU model, which has no internal memory, is the model that has
    the highest accuracy and shows the most contextual understanding in the
    connectivity analysis.
  </p>
  <p>
    It is entirely possible, that these results are highly dependent on the
    random initialization, hyperparameters, and the specific application. The
    importance of this article is therefore not to judge Nested LSTM overall
    but to show a better way of qualitatively comparing different RNNs in
    terms of their memorization abilities. In this case, the connectivity
    visualization has revealed much greater differences between the models,
    than one would guess from just looking at the accuracy and cross entropy
    loss alone.
  </p>
</dt-article>

<dt-appendix>
  <h3>Acknowledgments</h3>
  <p>
    Many thanks to the authors of the original Nested LSTM paper
    <dt-cite key="moniz2018nlstm"></dt-cite>, Joel Ruben, Antony Moniz,
    and David Krueger. Even though our findings weren't the same, they
    have inspired much of this article and shown that something as used
    as the recurrent unit is still an open research area.
  </p>
  <p>
    I'm also grateful for the excellent feedback and patience by
    Christopher Olah from the Distill team. His feedback has dramatically
    improved the quality of this article.
  </p>

  <h3 id="appendix-nestedlstm">Nested LSTM</h3>
  <p>
    The Nested LSTM unit attemps to solve the long-term memorization from a
    more practical point of view. Where the classic LSTM unit solves the
    vanishing gradient problem by adding internal memory, and the GRU attemps
    to be a faster solution than LSTM by using no internal memory, the Nested
    LSTM goes in the opposite direction of GRU by adding additional memory to
    the unit <dt-cite key="moniz2018nlstm"></dt-cite>.
    The idea here is that adding additional memory to the unit allows for more
    long-term memorization.
  </p>
  <figure>
    <img style="width: 100%;" src="graphics/nlstm-web.svg" alt="Nested Long Short Term Memory Diagram">
    <figcaption>
      <strong>Nested LSTM:</strong> makes the cell update depend on another
      LSTM unit, supposedly this allows more long-term memory compared to
      stacking LSTM layers.
    </figcaption>
  </figure>
  <p>
    The additional memory is integrated into the LSTM unit by changing how the
    cell value <math-latex latex="c_\ell^t"></math-latex> is updated. Instead of
    defining the cell value update as <math-latex latex="
      c_\ell^t = i_\ell^t \odot z_\ell^t + f_\ell^t \odot c_\ell^{t-1}
    "></math-latex>, as done in vanilla LSTM, it uses another LSTM unit:
    <math-latex display-mode latex="
      c_\ell^t = \mathrm{LSTM}(i_\ell^t \odot z_\ell^t, f_\ell^t \odot c_\ell^{t-1})
    "></math-latex>
    <span>See the defintion of <math-latex latex="
    \mathrm{LSTM}(\cdot, \cdot)"></math-latex> futher down <a href="#appendix-lstm">
    in the appendix</a>. </span>
  </p>
  <p>
    The complete set of equations then becomes:
    <math-latex display-mode latex="\begin{aligned}
      i_\ell^t &= \sigma_i(IW_{\ell-1, \ell} h_{\ell-1}^{t} + IW_{\ell, \ell} h_{\ell}^{t-1}) \\
      f_\ell^t &= \sigma_f(FW_{\ell-1, \ell} h_{\ell-1}^{t} + FW_{\ell, \ell} h_{\ell}^{t-1}) \\
      o_\ell^t &= \sigma_o(OW_{\ell-1, \ell} h_{\ell-1}^{t} + OW_{\ell, \ell} h_{\ell}^{t-1}) \\
      z_\ell^t &= \sigma_z(ZW_{\ell-1, \ell} h_{\ell-1}^{t} + ZW_{\ell, \ell} h_{\ell}^{t-1}) \\
      c_\ell^t &= \mathrm{LSTM}(i_\ell^t \odot z_\ell^t, f_\ell^t \odot c_\ell^{t-1}) \\
      h_{\ell}^{t} &= \mathrm{NLSTM}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \coloneqq o_\ell^t \odot \sigma_h(c_{\ell}^{t})
    \end{aligned}"></math-latex>
  </p>
  <p>
    Like in vanilla LSTM, the gate activation functions <math-latex latex="
    \left(\sigma_i(\cdot), \sigma_f(\cdot), \text{ and } \sigma_o(\cdot)\right)
    "></math-latex> are usually the simoid activation function. However,
    only the <math-latex latex="\sigma_h(\cdot)"></math-latex> is set to
    <math-latex latex="\tanh(\cdot)"></math-latex>. While,
    <math-latex latex="\sigma_z(\cdot)"></math-latex> is just the identity
    function, otherwise two non-linear activation functions would be applied
    on the same scalar without any change, except for the multiplication by
    the input gate. The activation functions for  <math-latex latex="
    \mathrm{LSTM}(\cdot, \cdot)"></math-latex> remains the same.
  </p>
  <p>
    The abstraction, of how to combine the input with the cell value, allows
    for a lot of flexibility. Using this abstraction, it is not only possible
    to add one extra internal memory state but the internal
    <math-latex latex="\mathrm{LSTM}(\cdot, \cdot)"></math-latex> unit can
    recursively be replaced by as many internal
    <math-latex latex="\mathrm{NLSTM}(\cdot, \cdot)"></math-latex> units as
    one would wish, thereby adding even more internal memory.
  </p>

  <h3 id="appendix-lstm">Long Short-Term Memory</h3>
  <p>
  The equations defining <math-latex latex="
  \mathrm{LSTM}(\cdot, \cdot)"></math-latex> as used in <math-latex latex="
  \mathrm{NLSTM}(\cdot, \cdot)"></math-latex> are:
  <math-latex display-mode latex="\begin{aligned}
    \tilde{i}_\ell^t &= \sigma_i(IW_{\ell-1, \ell} \tilde{h}_{\ell-1}^{t} + IW_{\ell, \ell} \tilde{h}_{\ell}^{t-1}) \\
    \tilde{f}_\ell^t &= \sigma_f(FW_{\ell-1, \ell} \tilde{h}_{\ell-1}^{t} + FW_{\ell, \ell} \tilde{h}_{\ell}^{t-1}) \\
    \tilde{o}_\ell^t &= \sigma_o(OW_{\ell-1, \ell} \tilde{h}_{\ell-1}^{t} + OW_{\ell, \ell} \tilde{h}_{\ell}^{t-1}) \\
    \tilde{z}_\ell^t &= \sigma_z(ZW_{\ell-1, \ell} \tilde{h}_{\ell-1}^{t} + ZW_{\ell, \ell} \tilde{h}_{\ell}^{t-1}) \\
    \tilde{c}_\ell^t &= \tilde{i}_\ell^t \odot \tilde{z}_\ell^t + \tilde{f}_\ell^t \odot \tilde{c}_\ell^{t-1} \\
    \tilde{h}_{\ell}^{t} &= \mathrm{LSTM}(\tilde{h}_{\ell-1}^{t}, \tilde{h}_{\ell}^{t-1}) \coloneqq \tilde{o}_\ell^t \odot \sigma_h(\tilde{c}_\ell^t)
  \end{aligned}"></math-latex>
  In terms of the Nested LSTM unit, <math-latex latex="
  \tilde{h}_{\ell-1}^{t} = i_\ell^t \odot z_\ell^t"></math-latex> and
  <math-latex latex="\tilde{h}_{\ell}^{t-1} = f_\ell^t \odot c_\ell^{t-1}"></math-latex>.
  </p>
  <p>
    The gate activation functions <math-latex latex="
    \left(\sigma_i(\cdot), \sigma_f(\cdot), \text{ and } \sigma_o(\cdot)\right)
    "></math-latex> are usually the simoid activation function.
    While <math-latex latex="
    \left(\sigma_z(\cdot) \text{ and } \sigma_h(\cdot)\right)
    "></math-latex> are usually <math-latex latex="\tanh(\cdot)"></math-latex>.
  </p>

  <h3 id="appendix-autocomplete">Autocomplete Problem</h3>
  <p>
    The autocomplete dataset is constructed form the full
    <a href="http://mattmahoney.net/dc/textdata.html">text8</a> dataset, where
    each observation consists of maximum 200 characters and is ensured to
    not contain partial words. 90% of the observations are used for training,
    5% for validation and 5% for testing.
  </p>
  <p>
    The input vocabulary is a-z, space, and a padding symbol. The output
    vocabulary consists of the <math-latex latex="2^{14} = 16384"></math-latex>
    most frequent words, and two additional symbols, one for padding and one
    for unknown words. The network is not penalized for predicting padding
    and unknown words wrong.
  </p>
  <p>
    The GRU, LSTM each have 2 layers of 600 units. Similarly, the Nested LSTM
    model has 1 layer of 600 units but with 2 internal memory states.
    Additionally, each model has an input embedding layer and a final dense
    layer to match the vocabulary size.
  </p>
  <figure class="ar-table-container">
    <table class="ar-table">
      <thead>
        <tr><th>Model</th><th>Units</th><th>Layers</th><th>Depth</th><th colspan="3">Parameters</th></tr>
        <tr><th></th><th></th><th></th><th></th><th>Embedding</th><th>Recurrent</th><th>Dense</th></tr>
      </thead>
      <tbody>
        <tr><td>GRU</td><td>600</td><td>2</td><td>N/A</td><td>16200</td><td>4323600</td><td>9847986</td>
        <tr><td>LSTM</td><td>600</td><td>2</td><td>N/A</td><td>16200</td><td>5764800</td><td>9847986</td>
        <tr><td>Nested LSTM</td><td>600</td><td>1</td><td>2</td><td>16200</td><td>5764800</td><td>9847986</td>
      </tbody>
    </table>
    <figcaption>
      <strong>Model Configurations:</strong> shows the number of layers, units and parameters
      for each model.
    </figcaption>
  </figure>
  <p>
    There are 456896 sequences in the training dataset and a batch size
    of 64 observations is used. A single iteration over the entire dataset
    then corresponds to 7139 epochs, which is enogth to train the network,
    therefore the models only trained for 7139 epochs. For training, Adam
    optimization is used with default parameters.
  </p>
  <figure>
    <svg id="ar-autocomplete-training" class="ar-training"></svg>
    <figcaption>
      <strong>Model training:</strong> shows the training loss and
      validation loss for the GRU, LSTM, and Nested LSTM models when training
      on the autocomplete problem. The x-axis is
      <a href="javascript:setTrainingGraphXAxis('time')">time</a> or
      <a href="javascript:setTrainingGraphXAxis('epochs')">epochs</a>.
    </figcaption>
  </figure>
</dt-appendix>

<script type="text/bibliography">
  @article{moniz2018nlstm,
    title={Nested LSTMs},
    author={Joel Ruben Antony Moniz and David Krueger},
    journal={arXivreprint arXiv:1801.10308},
    year={2018},
    url={https://arxiv.org/pdf/1801.10308.pdf}
  }

  @article{karpathy2015rnnvis,
    title={Visualizing and Understanding Recurrent Networks},
    author={Andrej Karpathy and Justin Johnson and Li Fei-Fei},
    journal={arXivreprint arXiv:1506.02078},
    year={2015},
    url={https://arxiv.org/pdf/1506.02078.pdf}
  }

  @article{graves2008phd,
    title={Supervised Sequence Labelling with Recurrent Neural Networks},
    author={Alex Graves},
    year={2008},
    url={https://www.cs.toronto.edu/~graves/phd.pdf}
  }

  @article{cho2014gru,
    title={Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
    author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
    journal={arXivreprint arXiv:1406.1078},
    year={2014},
    url={https://arxiv.org/pdf/1406.1078.pdf}
  }

  @article{graves2014turing,
    title={Neural Turing Machines},
    author={Alex Graves and Greg Wayne and Ivo Danihelka},
    journal={arXivreprint arXiv:1410.5401},
    year={2014},
    url={https://arxiv.org/pdf/1410.5401.pdf}
  }
</script>
