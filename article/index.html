<!doctype html>
<meta charset="utf-8">

<script src="https://distill.pub/template.v1.js"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">

<script defer src="bundle.js"></script>
<link href="style.css" rel="stylesheet">

<script type="text/front-matter">
  title: "Exploration of recurrent units in RNN"
  description: >
    An exploration of recent developments of Recurrent Units in RNN and their
    effect on contextual understanding in texts.
  authors:
  - Andreas Madsen: https://github.com/AndreasMadsen
  affiliations:
  - nearForm: https://nearform.com
</script>

<dt-article>
  <h1>Exploration of Recurrent Units in RNN</h1>
  <h2>An exploration of recent developments of recurrent units in RNN and their
  effect on contextual understanding in text.</h2>
  <figure id="ar-demo">
    <figcaption id="ar-demo-input-caption">User types input sequence.</figcaption>
    <div id="ar-demo-input-content"></div>

    <figcaption id="ar-demo-rnn-caption">Recurrent neural network processes the sequence.</figcaption>
    <div id="ar-demo-rnn-content"></div>

    <figcaption id="ar-demo-output-caption">The output for the last character is used.</figcaption>
    <div id="ar-demo-output-content"></div>

    <figcaption id="ar-demo-filter-caption">The most likely suggestions are extracted.</figcaption>
    <div id="ar-demo-filter-content"></div>

    <figcaption id="ar-demo-final-caption">The indices are looked up in a dictionary.</figcaption>
    <div id="ar-demo-final-content"></div>

    <figcaption id="ar-demo-caption">
      <strong>Autocomplete:</strong> An example application, showing how a simple
      recurrent neural network can be used for autocompletion. The network uses
      past information and understands the next word should be a country. Try
      <a href="javascript:arDemoShort();">removing the last letters</a>
      and see that the prediction uses contextual understanding
      (<a href="javascript:arDemoReset();">reset</a>).
    </figcaption>
  </figure>
  <dt-byline></dt-byline>
</dt-article>

<dt-article>
  <h2>Introduction</h2>
  <p>
    Recent advances in handwriting recognition, speech recognition
    <dt-cite key="hannun2015speach"></dt-cite>, and machine translation
    <dt-cite key="sutskever2014seq2seq"></dt-cite> have with only a few
    exceptions <dt-cite key="vaswani2017attention"></dt-cite>
    <dt-cite key="kalchbrenner2017bytenet"></dt-cite> been based on
    recurrent neural networks.
  </p>
  <p>
    These networks may use additional techniques such as attention mechanisms
    <dt-cite key="bahdanau2014attention"></dt-cite> to work with an unknown
    alignment between the source and the target sequence.
  </p>
  <p>
    However, the foundation for these networks is still the recurrent neural
    network. Likewise, a common challenge for many of these applications
    is to get the network to memorize past content from the input sequences
    and use this for contextual understanding later in the sequence.
  </p>
  <p>
    This memorization problem is what is explored in this article. To this end,
    this article doesn't go into the details of how to deal with an unknown
    alignment but rather focuses on problems where the alignment is known
    and explores the memorization issue for those problems.
  </p>
</dt-article>

<dt-article>
  <h2>Recurrent Units</h2>
  <p>
    Recurrent neural networks (RNNs) are well known and
    <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">
    thoroughly explained in literature</a>. To keep it short, recurrent
    neural networks allows one to model a sequence of vectors. RNNs do this
    by iterating over the sequence, where each layer uses the output from
    the same layer in the previous "time" iteration, combined with the output
    from the previous layer in the same "time" iteration.
  </p>
  <p>
    In theory, this type of network allows it in each iteration to know
    about every part of the sequence that came before.
  </p>
  <figure id="ar-recurrent-unit-rnn"></figure>
  <p>
    Given an input sequence <math-latex latex="\{x_t\}_{t=0}^T"></math-latex>,
    such model can be expressed using the following set of equations:
  <math-latex display-mode latex="\begin{aligned}
    h_{0}^t &= x_t \\
    h_{\ell}^{t} &= \mathrm{Unit}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \\
    y^t &= \mathrm{Softmax}(h_L^t) \\
    \mathcal{L} &= - \frac{1}{T} \sum_{t=1}^T \ln(y^t_k)
  \end{aligned}"></math-latex>
    Note how the output from the previous iteration (
    <math-latex latex="h_\ell^{t-1}"></math-latex>
    ) and the output from the previous layer in the same iteration (
    <math-latex latex="h_{\ell-1}^t"></math-latex>
    ) are combined, is abstracted away.
  </p>
  <p>
    For a vanilla recurrent neural network, the recurrent unit
    <math-latex latex="\mathrm{Unit}(\cdot, \cdot)"></math-latex>
    is:
  <math-latex display-mode latex="
    h_{\ell}^{t} = \mathrm{RNN}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \coloneqq \sigma(W_{\ell-1,\ell}h_{\ell-1}^t + W_{\ell,\ell}h_{\ell}^{t-1})
  "></math-latex>
  </p>
</dt-article>

<dt-article>
  <h2>Vanishing Gradient Problem</h2>
  <p>
    Deep neural networks can suffer from a vanishing gradient problem where
    the gradient used in optimization becomes minuscule. This is because the
    <math-latex latex="\delta_\ell"></math-latex> used in backpropagation
    ends up being multiplicatively depending on the
    <math-latex latex="\delta_\ell"></math-latex> of the next layer.
    <math-latex display-mode latex="
    \frac{\partial \mathcal{L}}{\partial h_\ell} = \delta_\ell = \sigma(z_\ell) \odot W_{\ell, \ell+1} \delta_{\ell+1}
    "></math-latex>
    This problem can be mitigated through careful initialization of the weights
    <math-latex latex="W_{\ell-1, \ell}"></math-latex>, by choosing an
    activation function <math-latex latex="\sigma"></math-latex>
    such as the Rectified Linear Unit (ReLU), or adding residual connections
    <dt-cite key="he2015resnet"></dt-cite>.
  </p>
  <figure id="ar-memorization-problem-rnn"></figure>
  <p>
    In classic recurrent neural networks, this problem becomes much worse,
    due to the time dependencies as the time dependencies essentially unfold
    into a potentially infinite deep neural network.
    <math-latex display-mode latex="
    \frac{\partial \mathcal{L}}{\partial h_\ell^t} = \delta_\ell^t = \sigma(z_\ell^t) \odot \left( W_{\ell, \ell+1} \delta_{\ell+1}^t +  W_{\ell, \ell} \delta_{\ell}^{t+1}\right)
    "></math-latex>
  </p>
  <p>
    An intutive way of viewing this problem is that the vanilla recurrent
    network forces an update of the state <math-latex latex="
    h_{\ell}^t"></math-latex>. This forced update, is what courses the vanishing
    gradient problem.
    This forced update is also insufficient as irrelevant input data, such as
    skip words, blur out important information from previous iterations.
  </p>
</dt-article>

<dt-article>
  <h2>Long Short-Term Memory</h2>
  <figure>
    <img src="graphics/lstm-web.svg" alt="Long Short Term Memory Diagram">
    <figcaption>
      <strong>LSTM:</strong> (Long Short-Term Memory) allows for long-term
      memorization by gateing its update, thereby solving the vanishing gradient
      problem.
    </figcaption>
  </figure>
  <p>
  The Long Short-Term Memory (LSTM) unit replaces the simple
  <math-latex latex="\mathrm{RNN}"></math-latex> unit from earlier. Each LSTM
  unit contains a single memory scalar that can be protected or written to,
  depending on the input and forget gate. This structure has shown to be
  very powerful in solving complex sequential problems
  <dt-cite key="graves2008phd"></dt-cite>. LSTM is well known and
  <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">
  thoroughly explained in the literature</a> and therefore not discussed here.
  However as it plays a critical part in the Nested LSTM unit, that is
  discussed later, its equations are mentioned here.
  <math-latex display-mode latex="\begin{aligned}
    i_\ell^t &= \sigma_i(IW_{\ell-1, \ell} h_{\ell-1}^{t} + IW_{\ell, \ell} h_{\ell}^{t-1}) \\
    f_\ell^t &= \sigma_f(FW_{\ell-1, \ell} h_{\ell-1}^{t} + FW_{\ell, \ell} h_{\ell}^{t-1}) \\
    o_\ell^t &= \sigma_o(OW_{\ell-1, \ell} h_{\ell-1}^{t} + OW_{\ell, \ell} h_{\ell}^{t-1}) \\
    z_\ell^t &= \sigma_z(ZW_{\ell-1, \ell} h_{\ell-1}^{t} + ZW_{\ell, \ell} h_{\ell}^{t-1}) \\
    c_\ell^t &= i_\ell^t \odot z_\ell^t + f_\ell^t \odot c_\ell^{t-1} \\
    h_{\ell}^{t} &= \mathrm{LSTM}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \coloneqq o_\ell^t \odot \sigma_h(c_\ell^t)
  \end{aligned}"></math-latex>
  </p>
  <p>
    The gate activation functions <math-latex latex="
    \left(\sigma_i(\cdot), \sigma_f(\cdot), \text{ and } \sigma_o(\cdot)\right)
    "></math-latex> are usually the simoid activation function.
    While <math-latex latex="
    \left(\sigma_z(\cdot) \text{ and } \sigma_h(\cdot)\right)
    "></math-latex> are usually <math-latex latex="\tanh(\cdot)"></math-latex>.
  </p>
</dt-article>

<dt-article>
  <h2>Gated Recurrent Unit</h2>
  <figure>
    <img src="graphics/gru-web.svg" alt="Gated Recurrent Unit Diagram">
    <figcaption>
      <strong>GRU:</strong> (Gated Recurrent Unit) also solves the vanishing
      gradient problem but does so without depending on an internal memory state.
    </figcaption>
  </figure>
  <p>
    The Gated Recurrent Unit (GRU) is a more recent advancement, as it was
    developed in 2014 <dt-cite key="cho2014gru"></dt-cite>. Like the LSTM Unit,
    it also solves the vanishing gradient problem. It is therefore similar to
    the LSTM Unit, however where the LSTM uses an internal memory state to
    control if the output equally its previous state, the GRU units
    controls this directly.
    <math-latex display-mode latex="\begin{aligned}
      r_\ell^t &= \sigma_r(RW_{\ell-1, \ell} h_{\ell-1}^{t} + RW_{\ell, \ell} h_{\ell}^{t-1}) \\
      u_\ell^t &= \sigma_u(UW_{\ell-1, \ell} h_{\ell-1}^{t} + UW_{\ell, \ell} h_{\ell}^{t-1}) \\
      z_\ell^t &= \sigma_z(ZW_{\ell-1, \ell} h_{\ell-1}^{t} + ZW_{\ell, \ell} (h_{\ell}^{t-1} \odot r_\ell^t)) \\
      h_{\ell}^{t} &= \mathrm{GRU}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \coloneqq (1 - u_\ell^t) \odot z_\ell^t + u_\ell^t \odot h_{\ell}^{t-1}
    \end{aligned}"></math-latex>
  </p>
  <p>
    The gate activation functions <math-latex latex="
    \left(\sigma_r(\cdot) \text{ and } \sigma_u(\cdot)\right)
    "></math-latex> are usually the simoid activation function,
    and <math-latex latex="\sigma_z(\cdot)"></math-latex> is usually
    <math-latex latex="\tanh(\cdot)"></math-latex>.
  </p>
</dt-article>

<dt-article>
  <h2>Nested LSTM</h2>
  <figure>
    <img src="graphics/nlstm-web.svg" alt="Nested Long Short Term Memory Diagram">
    <figcaption>
      <strong>Nested LSTM:</strong> makes the cell update depend on another
      LSTM unit, supposedly this allows more long-term memory compared to
      stacking LSTM layers.
    </figcaption>
  </figure>
  <p>
    Even though the LSTM unit and GRU solves the vanishing gradient problem on a
    theoretical level, long-term memorization continues to be a challenge in
    recurrent neural networks.
  </p>
  <p>
    The Nested LSTM unit attemps to solve the long-term memorization from a
    more practical point of view. Where the classic LSTM unit solves the
    vanishing gradient problem by adding internal memory, and the GRU attemps
    to be a faster solution than LSTM by using no internal memory, the Nested
    LSTM goes in the opposite direction of GRU by adding additional memory to
    the unit <dt-cite key="moniz2018nlstm"></dt-cite>.
  </p>
  <p>
    The idea here is that adding additional memory to the unit allows for more
    long-term memorization.
  </p>
  <p>
    The additional memory is integrated by changing how the cell value
    <math-latex latex="c_\ell^t"></math-latex> is updated. Instead of
    defining the cell value update as <math-latex latex="
      c_\ell^t = i_\ell^t \odot z_\ell^t + f_\ell^t \odot c_\ell^{t-1}
    "></math-latex>, it uses another LSTM unit:
    <math-latex display-mode latex="
      c_\ell^t = \mathrm{LSTM}(i_\ell^t \odot z_\ell^t, f_\ell^t \odot c_\ell^{t-1})
    "></math-latex>
    Note that the variables defined in
    <math-latex latex="\mathrm{LSTM}(\cdot, \cdot)"></math-latex> are different
    from those defined below. The end result is that an
    <math-latex latex="\mathrm{NLSTM}(\cdot, \cdot)"></math-latex> unit
    have two memory states.
  </p>
  <p>
    The complete set of equations then becomes:
    <math-latex display-mode latex="\begin{aligned}
      i_\ell^t &= \sigma_i(IW_{\ell-1, \ell} h_{\ell-1}^{t} + IW_{\ell, \ell} h_{\ell}^{t-1}) \\
      f_\ell^t &= \sigma_f(FW_{\ell-1, \ell} h_{\ell-1}^{t} + FW_{\ell, \ell} h_{\ell}^{t-1}) \\
      o_\ell^t &= \sigma_o(OW_{\ell-1, \ell} h_{\ell-1}^{t} + OW_{\ell, \ell} h_{\ell}^{t-1}) \\
      z_\ell^t &= \sigma_z(ZW_{\ell-1, \ell} h_{\ell-1}^{t} + ZW_{\ell, \ell} h_{\ell}^{t-1}) \\
      c_\ell^t &= \mathrm{LSTM}(i_\ell^t \odot z_\ell^t, f_\ell^t \odot c_\ell^{t-1}) \\
      h_{\ell}^{t} &= \mathrm{NLSTM}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \coloneqq o_\ell^t \odot \sigma_h(c_{\ell}^{t})
    \end{aligned}"></math-latex>
  </p>
  <p>
    Like in vanilla LSTM, the gate activation functions <math-latex latex="
    \left(\sigma_i(\cdot), \sigma_f(\cdot), \text{ and } \sigma_o(\cdot)\right)
    "></math-latex> are usually the simoid activation function. However,
    only the <math-latex latex="\sigma_h(\cdot)"></math-latex> is set to
    <math-latex latex="\tanh(\cdot)"></math-latex>. While,
    <math-latex latex="\sigma_z(\cdot)"></math-latex> is just the identity
    function, otherwise two non-linear activation functions would be applied
    on the same scalar without any change, except for the multiplication by
    the input gate. The activation functions for  <math-latex latex="
    \mathrm{LSTM}(\cdot, \cdot)"></math-latex> remains the same.
  </p>
  <p>
    The abstraction, of how to combine the input with the cell value, allows
    a lot of flexibility. Using this abstraction, it is not only possible
    to add one extra internal memory state but the internal
    <math-latex latex="\mathrm{LSTM}(\cdot, \cdot)"></math-latex> unit can
    recursively be replaced as many internal
    <math-latex latex="\mathrm{NLSTM}(\cdot, \cdot)"></math-latex> units as
    one would wish, thereby adding even more internal memory.
  </p>
  <p>
    From a theoretical view, whether or not the Nested LSTM unit improves long-term
    memorization is not really clear. The LSTM unit theoretically solves the vanishing
    gradient problem and a network of LSTM units is Turing complete. In theory,
    using LSTM units should be sufficient for solving problems that require
    long-term memorization.
  </p>
  <p>
    That being said, it is often very difficult to train LSTM and GRU based
    recurrent neural networks. These difficulties often come down to the
    curvature of the loss function and it is possible that the Nested LSTM
    improves this curvature and therefore is easier to optimize.
  </p>
</dt-article>

<dt-article>
  <h2>Comparing Recurrent Units</h2>
  <p>
    Comparing the different Recurrent Units is not a trivial task. Different
    problem requires different contextual understanding and therefore requires
    different memorization.
  </p>
  <p>
    A good problem for analyzing the contextual understanding, should have
    a humanly interpretive output and depend both on long and short memorization.
  </p>
  <p>
    To this end, the autocomplete problem is used. Each character is mapped
    to a target that represents the entire word. To make it extra difficult,
    the space leading up to the word should also map to that word.
  </p>
  <p>
    The autocomplete problem is not a standard problem, thus the text8 problem,
    where each character will predict the next character, is also used. This
    is a very similar problem to the autocomplete problem but it is not as
    easy to interpret how meaningful the output is.
  </p>
  <h3>Autocomplete Problem</h3>
  <p>
    The input vocabulary is a-z, space, and a padding symbol. The output
    vocabulary consists of the <math-latex latex="2^{14} = 16384"></math-latex>
    most frequent words, and two additional symbols, one for padding and one
    for unknown words. The network is not penalized for predicting padding
    and unknown words wrong.
  </p>
  <p>
    The dataset is the full
    <a href="http://mattmahoney.net/dc/textdata.html">text8</a> dataset, where
    each observation consists of maximum 200 characters and is ensured to
    not contain partial words. 90% of the observations are used for training,
    5% for validation and 5% for testing.
  </p>
  <p>
    The GRU, LSTM each have 2 layers of 600 units. Similarly, the Nested LSTM
    model has 1 layer of 600 units but with 2 internal memory states.
    Additionally, each model has an input embedding layer and a final dense
    layer to match the vocabulary size.
  </p>
  <figure class="ar-table-container">
    <table class="ar-table">
      <thead>
        <tr><th>Model</th><th>Units</th><th>Layers</th><th>Depth</th><th colspan="3">Parameters</th></tr>
        <tr><th></th><th></th><th></th><th></th><th>Embedding</th><th>Recurrent</th><th>Dense</th></tr>
      </thead>
      <tbody>
        <tr><td>GRU</td><td>600</td><td>2</td><td>N/A</td><td>16200</td><td>4323600</td><td>9847986</td>
        <tr><td>LSTM</td><td>600</td><td>2</td><td>N/A</td><td>16200</td><td>5764800</td><td>9847986</td>
        <tr><td>Nested LSTM</td><td>600</td><td>1</td><td>2</td><td>16200</td><td>5764800</td><td>9847986</td>
      </tbody>
    </table>
    <figcaption>
      <strong>Model Configurations:</strong> shows the number of layers, units and parameters
      for each model.
    </figcaption>
  </figure>
  <p>
    There are 508583 sequences in the training dataset and a batch size
    of 64 observations is used. A single iteration over the entire dataset
    then corresponds to 7946 epochs, which is enogth to train the network,
    therefore the models only trained for 7946 epochs. For training, Adam
    optimization is used with default parameters.
  </p>
  <figure>
    <svg id="ar-autocomplete-training" class="ar-training"></svg>
    <figcaption>
      <strong>Model training:</strong> shows the training loss and
      validation loss for the GRU, LSTM, and Nested LSTM models when training
      on the autocomplete problem.
    </figcaption>
  </figure>
  <figure class="ar-table-container">
    <table class="ar-table">
      <thead>
        <tr><th>Model</th><th>Cross Entropy</th><th>Accuracy</th></tr>
      </thead>
      <tbody>
        <tr><td>GRU</td><td>2.1497</td><td>51.61%</td>
        <tr><td>LSTM</td><td>2.2899</td><td>49.90%</td>
        <tr><td>Nested LSTM</td><td>2.6051</td><td>45.47%</td>
      </tbody>
    </table>
    <figcaption>
      <strong>Model testing:</strong> shows the testing loss and accuracy
      for the GRU, LSTM, and Nested LSTM models on the autocomplete problem.
    </figcaption>
  </figure>
  <p>
    As seen from the results the models are more or less equally fast.
    Surprisingly the Nested LSTM is not better than the LSTM or GRU models.
    This somewhat contradicts the results found in the Nested LSTM paper
    <dt-cite key="moniz2018nlstm"></dt-cite>, although they tested model
    on different problems and therefore the results are not exactly comparable.
    Never or less one would still expect the Nested LSTM model to perform
    better for this problem, where long-term memorization is important for
    the contextual understanding.
  </p>
  <p>
    An unexpected result is that the Nested LSTM model initially
    converges much faster than the LSTM and GRU models. This, combined with
    the worse performance, indicates that the Nested LSTM optimizes forwards
    an unideal local minimum.
  </p>
  <h3>Connectivity in the Autocomplete Problem</h3>
  <p>
  To get a better idea of how well each model memorizes and uses that for
  contextual understanding, the connectivity between the desired output and
  the input should be analyzed. This is calculated as the magnitude of
  the gradient, between the logits for the desired output
  <math-latex latex="(h_L^t)_k"></math-latex> and the input
  <math-latex latex="x^t"></math-latex>.
  <math-latex display-mode latex="
    \textrm{connectivity}(t, \tilde{t}) =
    \left|\left|\frac{\partial (h_L^{\tilde{t}})_k}{\partial x^t}\right|\right|_2
  "></math-latex>
  </p>
  <figure>
    <div id="ar-connectivity-nlstm" class="ar-connectivity"></div>
    <div id="ar-connectivity-lstm" class="ar-connectivity"></div>
    <div id="ar-connectivity-gru" class="ar-connectivity"></div>
    <figcaption>
      <strong>Connectivity:</strong> shows the connection strength between
      the target for the selected character and the input characters
      (<a href="javascript:connectivitySetIndex(null);">reset</a>).
    </figcaption>
  </figure>
  <p>
    Exploring the connectivity gives a surprising amount of insight into the
    different models ability for long-term contextual understanding. You should
    try and interact with the figure yourself, to see what information the
    different models use.
  </p>
  <p>
    Here are two interesting observations:
  </p>
  <ul>
    <li>
  The first observation, is when the <a href="javascript:connectivitySetIndex(105);">
  models should predict "learning" and is only given data
  until the first character</a>. Here it is clear, that the Nested LSTM
  model uses no past information and thus only suggests common words starting
  with the letter "l".<br>

  The LSTM model suggests variations on the word "language",
  which makes sense given the text is about learning grammar. The GRU model
  does one better, by using the words "study" and "education" to also suggests
  the correct word "learning".
    </li>
    <li>
  The second observation, is when the models should predict the word "grammar".
  This word appears twice, for the first case very little is known
  about the context. Thus, the models don't suggest "grammar" until they have
  <a href="javascript:connectivitySetIndex(31);">seen 3</a> or
  <a href="javascript:connectivitySetIndex(32);">4 characters</a>.<br>

  For the second occurrence, there is much more text for the models to
  understand the context of the word. The GRU model is thus able to predict
  the word "grammar" with only <a href="javascript:connectivitySetIndex(149);">
  1 character from the word itself</a>. While the LSTM and Nested LSTM again
  needs <a href="javascript:connectivitySetIndex(162);">4 characters</a>.
    </li>
  </ul>
  <p>
    These observations suggest that the Nested LSTM model, in particular,
    doesn't use long-term memorization as one would otherwise expect from the
    additional memory states.
  </p>
  <h3>Text8 Generation Problem</h3>
  <p>
    The Text8 generation problem is a more commonly used problem. From the
    <a href="http://mattmahoney.net/dc/textdata.html">text8</a> dataset, each
    character should predict the next character. The dataset is created
    similarly to the autocomplete dataset. No partial words are allowed,
    90% for training data, 5% for validation, 5% for testing. The only difference
    is that the max length is 180 characters, which is to match previous
    word <dt-cite key="moniz2018nlstm"></dt-cite>.
  </p>
  <p>
    The models are also identical, except 1200 units are used. Again, this is
    to match previous work <dt-cite key="moniz2018nlstm"></dt-cite>.
  </p>
  <figure class="ar-table-container">
    <table class="ar-table">
      <thead>
        <tr><th>Model</th><th>Units</th><th>Layers</th><th>Depth</th><th colspan="3">Parameters</th></tr>
        <tr><th></th><th></th><th></th><th></th><th>Embedding</th><th>Recurrent</th><th>Dense</th></tr>
      </thead>
      <tbody>
        <tr><td>GRU</td><td>1200</td><td>2</td><td>N/A</td><td>33600</td><td>17287200</td><td>34829</td>
        <tr><td>LSTM</td><td>1200</td><td>2</td><td>N/A</td><td>33600</td><td>23049600</td><td>34829</td>
        <tr><td>Nested LSTM</td><td>1200</td><td>1</td><td>2</td><td>33600</td><td>23049600</td><td>34829</td>
      </tbody>
    </table>
    <figcaption>
      <strong>Model Configurations:</strong> shows the number of layers, units,
      and parameters for each model.
    </figcaption>
  </figure>
  <p>
    As there are 508583 sequences in the training dataset, the models only
    train over the dataset once, which is 7946 epochs. For training, Adam
    optimization is used with default parameters. Unlike previous work,
    gradient clipping isn't used as it didn't appear to provide any benefit.
  </p>
  <figure>
    <svg id="ar-generate-training" class="ar-training"></svg>
    <figcaption>
      <strong>Model training:</strong> shows the training loss and
      validation loss for the GRU, LSTM, and Nested LSTM models when training
      on the text8 generation problem.
    </figcaption>
  </figure>
  <figure class="ar-table-container">
    <table class="ar-table">
      <thead>
        <tr><th>Model</th><th>Cross Entropy</th><th>Accuracy</th></tr>
      </thead>
      <tbody>
        <tr><td>GRU</td><td>1.1212</td><td>64.80%</td>
        <tr><td>LSTM</td><td>1.0680</td><td>66.35%</td>
        <tr><td>Nested LSTM</td><td>1.1135</td><td>64.79%</td>
      </tbody>
    </table>
    <figcaption>
      <strong>Model testing:</strong> shows the testing loss and accuracy
      for the GRU, LSTM, and Nested LSTM models on the text8 generation problem.
    </figcaption>
  </figure>
  <p>
    The results show that the models are more or less equivalent, with the
    LSTM model performing only marginally better than the GRU, and Nested LSTM
    models. It was thus not possible to replicate the results form the original
    Nested LSTM article <dt-cite key="moniz2018nlstm"></dt-cite>.
  </p>
</dt-article>

<dt-article>
  <h2>Conclusion</h2>
  <p>
    Looking at accuracy and cross entropy loss in itself is not that
    interesting. A reasonable high accuracy can be obtained, with reasonably
    low contextual understanding. It is only for the first couple of characters,
    that long-term memorization and contextual understanding really matters.
  </p>
  <p>
    A more qualitative analysis of the connectivity together with the prediction,
    reveals that the Nested LSTM model isn't capable of long-term contextual
    understanding. This contradicts the design decisions for the model and the
    findings in the original paper <dt-cite key="moniz2018nlstm"></dt-cite>.
    In fact, the GRU model, which has no internal memory, is the model that
    has the highest accuracy and shows the most contextual understanding in the
    connectivity analysis.
  </p>
  <p>
    It is entirely possible, that the results are highly dependent on the
    random initialization and the specific problem. However, the results still
    show that more internal memory per recurrent unit does not necessarily
    improve long-term memorization.
  </p>
</dt-article>

<dt-appendix>
  <h3>Acknowledgments</h3>
  <p>
    Many thanks to the authors of the original Nested LSTM paper
    <dt-cite key="moniz2018nlstm"></dt-cite>, Joel Ruben, Antony Moniz,
    and David Krueger. Even though our findings weren't the same, they
    have inspired much of this article and shown that something as used
    as the recurrent unit is still an open research area.
  </p>
</dt-appendix>

<script type="text/bibliography">
  @article{hannun2015speach,
    title={Deep Speech: Scaling up end-to-end speech recognition},
    author={Awni Hannun and Carl Case and Jared Casper and Bryan Catanzaro and Greg Diamos and Erich Elsen and Ryan Prenger and Sanjeev Satheesh and Shubho Sengupta and Adam Coates and Andrew Y. Ng},
    journal={arXivreprint arXiv:1412.5567},
    year={2014},
    url={https://arxiv.org/pdf/1412.5567.pdf}
  }

  @article{sutskever2014seq2seq,
    title={Sequence to Sequence Learning with Neural Networks},
    author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
    journal={arXivreprint arXiv:1409.3215},
    year={2014},
    url={https://arxiv.org/pdf/1409.3215.pdf}
  }

  @article{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    journal={arXivreprint arXiv:1706.03762},
    year={2017},
    url={https://arxiv.org/pdf/1706.03762.pdf}
  }

  @article{kalchbrenner2017bytenet,
    title={Neural Machine Translation in Linear Time},
    author={Nal Kalchbrenner and Lasse Espeholt and Karen Simonyan and Aaron van den Oord and Alex Gravesand Koray Kavukcuoglu},
    journal={arXivreprint arXiv:1610.10099},
    year={2017},
    url={https://arxiv.org/pdf/1610.10099.pdf}
  }

  @article{bahdanau2014attention,
    title={Neural Machine Translation by Jointly Learning to Align and Translate},
    author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
    journal={arXivreprint arXiv:1409.0473},
    year={2014},
    url={https://arxiv.org/pdf/1409.0473.pdf}
  }

  @article{moniz2018nlstm,
    title={Nested LSTMs},
    author={Joel Ruben Antony Moniz and David Krueger},
    journal={arXivreprint arXiv:1801.10308},
    year={2018},
    url={https://arxiv.org/pdf/1801.10308.pdf}
  }

  @article{graves2008phd,
    title={Supervised Sequence Labelling with Recurrent Neural Networks},
    author={Alex Graves},
    year={2008},
    url={https://www.cs.toronto.edu/~graves/phd.pdf}
  }

  @article{cho2014gru,
    title={Supervised Sequence Labelling with Recurrent Neural Networks},
    author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
    journal={arXivreprint arXiv:1406.1078},
    year={2014},
    url={https://arxiv.org/pdf/1406.1078.pdf}
  }

  @article{he2015resnet,
    title={Supervised Sequence Labelling with Recurrent Neural Networks},
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    journal={arXivreprint arXiv:1512.03385},
    year={2015},
    url={https://arxiv.org/pdf/1512.03385.pdf}
  }
</script>
