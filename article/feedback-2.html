<!doctype html>
<meta charset="utf-8">

<script src="https://distill.pub/template.v1.js"></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">

<script defer src="feedback-bundle.js"></script>
<link href="feedback-style.css" rel="stylesheet">

<script type="text/front-matter">
  title: "The effect of internal memory in RNNs"
  description: >
    A visualization method for contextual understanding in text, used for
    comparing the effect of the number of internal memory states in different
    recurrent units.
  authors:
  - Andreas Madsen: https://github.com/AndreasMadsen
  affiliations:
  - nearForm: https://nearform.com
</script>

<dt-article>
  <h1>The effect of internal memory in RNNs</h1>
  <h2>A visualization method for contextual understanding in text, used for
  comparing the effect of the number of internal memory states in different
  recurrent units.
  </h2>
  <figure id="ar-demo">
    <figcaption id="ar-demo-input-caption">User types input sequence.</figcaption>
    <div id="ar-demo-input-content"></div>

    <figcaption id="ar-demo-rnn-caption">Recurrent neural network processes the sequence.</figcaption>
    <div id="ar-demo-rnn-content"></div>

    <figcaption id="ar-demo-output-caption">The output for the last character is used.</figcaption>
    <div id="ar-demo-output-content"></div>

    <figcaption id="ar-demo-final-caption">The most likely suggestions are extracted.</figcaption>
    <div id="ar-demo-final-content"></div>

    <figcaption id="ar-demo-caption">
      <strong>Autocomplete:</strong> An example application, showing how a simple
      recurrent neural network can be used for autocompletion. The network uses
      past information and understands the next word should be a country. Try
      <a href="javascript:arDemoShort();">removing the last letters</a>
      to see that the network understand that the next word should be a country.
      This example shows the network is capable of contextual understanding.
      (<a href="javascript:arDemoReset();">reset</a>).
    </figcaption>
  </figure>
  <dt-byline></dt-byline>
</dt-article>

<dt-article>
  <h2>Introduction</h2>
  <p>
    Memorization in Recurrent Neural Networks continues to pose a challenge
    in many applications. The Long-Short-Term Memory (LSTM) unit and Gated
    Recurrent Unit (GRU) are typically used to solve this problem, by
    solving the vanishing gradient problem. However, the practical problem
    of memorization still poses a challenge.
  </p>
  <p>
    Recently a new paper by Monzi et. al <dt-cite key="moniz2018nlstm"></dt-cite>
    have proposed an extension of the LSTM unit, called Nested LSTM. In that
    paper, they visualize an individual cell activation to show that the
    Nested LSTM unit has a superior ability to memorize.
  </p>
  <p>
    This visualization is inspired by another paper
    <dt-cite key="karpathy2015rnnvis"></dt-cite> where they identify cells
    that captures a specific feature. This visualization approach is good
    for identifying specific features. However, it is not a good argument for
    memorization in general as the output is entirely dependent on which feature
    the specific cell captures.
  </p>
  <p>
    This article proposes a better visualization method of connectivity for
    showing memorization and contextual understanding. Furthermore, the
    autocomplete problem is used as the main problem, as the output is much
    easier to reason about compared to more standard problems such as
    Penn Treebank, Chinese Poetry Generation, or text8 generation.
  </p>
  <p>
    It is the hope that the connectivity visualization used on the
    autocomplete problem, will help advance the dialogue around contextual
    understanding in recurrent units.
  </p>
</dt-article>

<dt-article>
  <h2>Recurrent Units</h2>
  <p>
    Recurrent neural networks (RNNs) are well known and
    <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">
    thoroughly explained in literature</a>. To keep it short, recurrent
    neural networks allows one to model a sequence of vectors. RNNs do this
    by iterating over the sequence, where each layer uses the output from
    the same layer in the previous "time" iteration, combined with the output
    from the previous layer in the same "time" iteration.
  </p>
  <p>
    Given an input sequence <math-latex latex="\{x_t\}_{t=0}^T"></math-latex>,
    such model can be expressed using the following set of equations:
  <math-latex display-mode latex="\begin{aligned}
    h_{\ell}^{t} &= \mathrm{Unit}(h_{\ell-1}^{t}, h_{\ell}^{t-1}), \text{ where: } h_{0}^t = x_t \\
    y^t &= \mathrm{Softmax}(h_L^t) \\
    \mathcal{L} &= - \frac{1}{T} \sum_{t=1}^T \ln(y^t_k)
  \end{aligned}"></math-latex>
    Note how the output from the previous iteration (
    <math-latex latex="h_\ell^{t-1}"></math-latex>
    ) and the output from the previous layer in the same iteration (
    <math-latex latex="h_{\ell-1}^t"></math-latex>
    ) are combined, it is abstracted away. This abstraction is the foundation
    for the Nested LSTM unit which is explained later.
  </p>
  <p>
    For a vanilla recurrent neural network, the recurrent unit
    <math-latex latex="\mathrm{Unit}(\cdot, \cdot)"></math-latex>
    is:
  <math-latex display-mode latex="
    h_{\ell}^{t} = \mathrm{RNN}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \coloneqq \sigma(W_{\ell-1,\ell}h_{\ell-1}^t + W_{\ell,\ell}h_{\ell}^{t-1})
  "></math-latex>
  </p>
  <p>
    In theory, the time dependency allows it in each iteration to know
    about every part of the sequence that came before. However, because
    of this time dependency it is very common for the network to suffer from a
    vanishing gradient problem, that causes the training to ignore long-term
    dependencies.
  </p>
  <figure id="ar-memorization-problem-rnn">
    <figcaption>
      <strong>Vanishing Gradient: </strong> where the contribution from the
      earlier steps becomes insignificant in the gradient:
      <math-latex latex="
      \frac{\partial \mathcal{L}}{\partial h_\ell^t} = \delta_\ell^t = \sigma(z_\ell^t) \odot \left( W_{\ell, \ell+1} \delta_{\ell+1}^t +  W_{\ell, \ell} \delta_{\ell}^{t+1}\right)
      "></math-latex>
    </figcaption>
  </figure>
  <p>
    The Long Short-Term Memory (LSTM) unit and Gated Recurret Unit (GRU)
    solves this vanishing gradient problem by using gates. These gates control
    how much the new state value should be updated and thereby allows it
    to selectively memorize <dt-cite key="graves2008phd"></dt-cite>
    <dt-cite key="cho2014gru"></dt-cite>. Both LSTM and GRU are well known and
    also <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">
    thoroughly explained in the literature</a> and therefore not discussed here.
  </p>
</dt-article>

<dt-article>
  <h2>Nested LSTM</h2>
  <figure>
    <img src="graphics/nlstm-feedback-1-web.svg" alt="Nested Long Short Term Memory Diagram">
    <figcaption>
      <strong>Nested LSTM:</strong> makes the cell update depend on another
      LSTM unit, supposedly this allows more long-term memory compared to
      stacking LSTM layers.
    </figcaption>
  </figure>
  <p>
    Even though the LSTM unit and GRU solves the vanishing gradient problem on a
    theoretical level, long-term memorization continues to be a challenge in
    recurrent neural networks.
  </p>
  <p>
    The Nested LSTM unit attemps to solve the long-term memorization from a
    more practical point of view. Where the classic LSTM unit solves the
    vanishing gradient problem by adding internal memory, and the GRU attemps
    to be a faster solution than LSTM by using no internal memory, the Nested
    LSTM goes in the opposite direction of GRU by adding additional memory to
    the unit <dt-cite key="moniz2018nlstm"></dt-cite>.
  </p>
  <p>
    The idea here is that adding additional memory to the unit allows for more
    long-term memorization.
  </p>
  <p>
    The additional memory is integrated into the LSTM unit by changing how the
    cell value <math-latex latex="c_\ell^t"></math-latex> is updated. Instead of
    defining the cell value update as <math-latex latex="
      c_\ell^t = i_\ell^t \odot z_\ell^t + f_\ell^t \odot c_\ell^{t-1}
    "></math-latex>, as done in vanilla LSTM, it uses another LSTM unit:
    <math-latex display-mode latex="
      c_\ell^t = \mathrm{LSTM}(i_\ell^t \odot z_\ell^t, f_\ell^t \odot c_\ell^{t-1})
    "></math-latex>
    <span>See the defintion of <math-latex latex="
    \mathrm{LSTM}(\cdot, \cdot)"></math-latex> <a href="#appendix-lstm">
    in the appendix</a>. </span>
  </p>
  <p>
    The complete set of equations then becomes:
    <math-latex display-mode latex="\begin{aligned}
      i_\ell^t &= \sigma_i(IW_{\ell-1, \ell} h_{\ell-1}^{t} + IW_{\ell, \ell} h_{\ell}^{t-1}) \\
      f_\ell^t &= \sigma_f(FW_{\ell-1, \ell} h_{\ell-1}^{t} + FW_{\ell, \ell} h_{\ell}^{t-1}) \\
      o_\ell^t &= \sigma_o(OW_{\ell-1, \ell} h_{\ell-1}^{t} + OW_{\ell, \ell} h_{\ell}^{t-1}) \\
      z_\ell^t &= \sigma_z(ZW_{\ell-1, \ell} h_{\ell-1}^{t} + ZW_{\ell, \ell} h_{\ell}^{t-1}) \\
      c_\ell^t &= \mathrm{LSTM}(i_\ell^t \odot z_\ell^t, f_\ell^t \odot c_\ell^{t-1}) \\
      h_{\ell}^{t} &= \mathrm{NLSTM}(h_{\ell-1}^{t}, h_{\ell}^{t-1}) \coloneqq o_\ell^t \odot \sigma_h(c_{\ell}^{t})
    \end{aligned}"></math-latex>
  </p>
  <p>
    Like in vanilla LSTM, the gate activation functions <math-latex latex="
    \left(\sigma_i(\cdot), \sigma_f(\cdot), \text{ and } \sigma_o(\cdot)\right)
    "></math-latex> are usually the simoid activation function. However,
    only the <math-latex latex="\sigma_h(\cdot)"></math-latex> is set to
    <math-latex latex="\tanh(\cdot)"></math-latex>. While,
    <math-latex latex="\sigma_z(\cdot)"></math-latex> is just the identity
    function, otherwise two non-linear activation functions would be applied
    on the same scalar without any change, except for the multiplication by
    the input gate. The activation functions for  <math-latex latex="
    \mathrm{LSTM}(\cdot, \cdot)"></math-latex> remains the same.
  </p>
  <p>
    The abstraction, of how to combine the input with the cell value, allows
    for a lot of flexibility. Using this abstraction, it is not only possible
    to add one extra internal memory state but the internal
    <math-latex latex="\mathrm{LSTM}(\cdot, \cdot)"></math-latex> unit can
    recursively be replaced by as many internal
    <math-latex latex="\mathrm{NLSTM}(\cdot, \cdot)"></math-latex> units as
    one would wish, thereby adding even more internal memory.
  </p>
  <p>
    From a theoretical view, whether or not the Nested LSTM unit improves long-term
    memorization is not really clear. The LSTM unit theoretically solves the vanishing
    gradient problem and a network of LSTM units is Turing complete. Therefore,
    in theory, using LSTM units should be sufficient for solving problems that
    require long-term memorization.
  </p>
  <p>
    That being said, it is often very difficult to train LSTM and GRU based
    recurrent neural networks. These difficulties often come down to the
    curvature of the loss function and it is possible that the Nested LSTM
    improves this curvature and therefore is easier to optimize.
  </p>
</dt-article>

<dt-article>
  <h2>Comparing Recurrent Units</h2>
  <p>
    Comparing the different Recurrent Units is not a trivial task. Different
    problem requires different contextual understanding and therefore requires
    different memorization.
  </p>
  <p>
    A good problem for qualitatively analyzing the contextual understanding,
    should have a humanly interpretable output and depend both on long and
    short memorization.
  </p>
  <p>
    To this end, the autocomplete problem is used. Each character is mapped
    to a target that represents the entire word. To make it extra difficult,
    the space leading up to the word should also map to that word. This
    prediction, is in particular useful for showing contextual understanding.
  </p>
  <h3>Autocomplete Problem</h3>
  <p>
    The input vocabulary is a-z, space, and a padding symbol. The output
    vocabulary consists of the <math-latex latex="2^{14} = 16384"></math-latex>
    most frequent words, and two additional symbols, one for padding and one
    for unknown words. The network is not penalized for predicting padding
    and unknown words wrong.
  </p>
  <p>
    The dataset is the full
    <a href="http://mattmahoney.net/dc/textdata.html">text8</a> dataset, where
    each observation consists of maximum 200 characters and is ensured to
    not contain partial words. 90% of the observations are used for training,
    5% for validation and 5% for testing.
  </p>
  <p>
    The GRU, LSTM each have 2 layers of 600 units. Similarly, the Nested LSTM
    model has 1 layer of 600 units but with 2 internal memory states.
    Additionally, each model has an input embedding layer and a final dense
    layer to match the vocabulary size.
  </p>
  <figure class="ar-table-container">
    <table class="ar-table">
      <thead>
        <tr><th>Model</th><th>Units</th><th>Layers</th><th>Depth</th><th colspan="3">Parameters</th></tr>
        <tr><th></th><th></th><th></th><th></th><th>Embedding</th><th>Recurrent</th><th>Dense</th></tr>
      </thead>
      <tbody>
        <tr><td>GRU</td><td>600</td><td>2</td><td>N/A</td><td>16200</td><td>4323600</td><td>9847986</td>
        <tr><td>LSTM</td><td>600</td><td>2</td><td>N/A</td><td>16200</td><td>5764800</td><td>9847986</td>
        <tr><td>Nested LSTM</td><td>600</td><td>1</td><td>2</td><td>16200</td><td>5764800</td><td>9847986</td>
      </tbody>
    </table>
    <figcaption>
      <strong>Model Configurations:</strong> shows the number of layers, units and parameters
      for each model.
    </figcaption>
  </figure>
  <p>
    There are 456896 sequences in the training dataset and a batch size
    of 64 observations is used. A single iteration over the entire dataset
    then corresponds to 7139 epochs, which is enogth to train the network,
    therefore the models only trained for 7139 epochs. For training, Adam
    optimization is used with default parameters.
  </p>
  <figure>
    <svg id="ar-autocomplete-training" class="ar-training"></svg>
    <figcaption>
      <strong>Model training:</strong> shows the training loss and
      validation loss for the GRU, LSTM, and Nested LSTM models when training
      on the autocomplete problem. The x-axis is
      <a href="javascript:setTrainingGraphXAxis('time')">time</a> or
      <a href="javascript:setTrainingGraphXAxis('epochs')">epochs</a>.
    </figcaption>
  </figure>
  <figure class="ar-table-container">
    <table class="ar-table">
      <thead>
        <tr><th>Model</th><th>Cross Entropy</th><th>Accuracy</th></tr>
      </thead>
      <tbody>
        <tr><td>GRU</td><td>2.1497</td><td>51.61%</td>
        <tr><td>LSTM</td><td>2.2899</td><td>49.90%</td>
        <tr><td>Nested LSTM</td><td>2.6051</td><td>45.47%</td>
      </tbody>
    </table>
    <figcaption>
      <strong>Model testing:</strong> shows the testing loss and accuracy
      for the GRU, LSTM, and Nested LSTM models on the autocomplete problem.
    </figcaption>
  </figure>
  <p>
    As seen from the results the models are more or less equally fast.
    Surprisingly the Nested LSTM is not better than the LSTM or GRU models.
    This somewhat contradicts the results found in the Nested LSTM paper
    <dt-cite key="moniz2018nlstm"></dt-cite>, although they tested the models
    on different problems and therefore the results are not exactly comparable.
    Never or less one would still expect the Nested LSTM model to perform
    better for this problem, where long-term memorization is important for
    the contextual understanding.
  </p>
  <h3>Connectivity in the Autocomplete Problem</h3>
  <p>
  To get a better idea of how well each model memorizes and uses memory for
  contextual understanding, the connectivity between the desired output and
  the input should be analyzed. This is calculated as the magnitude of
  the gradient, between the logits for the desired output
  <math-latex latex="(h_L^{\tilde{t}})_k"></math-latex> and the input
  <math-latex latex="x^t"></math-latex>.
  <math-latex display-mode latex="
    \textrm{connectivity}(t, \tilde{t}) =
    \left|\left|\frac{\partial (h_L^{\tilde{t}})_k}{\partial x^t}\right|\right|_2
  "></math-latex>
  </p>
  <figure>
    <div id="ar-connectivity-nlstm" class="ar-connectivity"></div>
    <div id="ar-connectivity-lstm" class="ar-connectivity"></div>
    <div id="ar-connectivity-gru" class="ar-connectivity"></div>
    <figcaption>
      <strong>Connectivity:</strong> shows the connection strength between
      the target for the selected character and the input characters
      (<a href="javascript:connectivitySetIndex(null);">reset</a>).
    </figcaption>
  </figure>
  <p>
    Exploring the connectivity gives a surprising amount of insight into the
    different models ability for long-term contextual understanding. You should
    try and interact with the figure yourself, to see what information the
    different models use.
  </p>
  <p>
    Here are two interesting observations:
  </p>
  <ul>
    <li>
  The first observation, is when the <a href="javascript:connectivitySetIndex(105);">
  models should predict "learning" and is only given data
  until the first character</a>. Here it is clear, that the Nested LSTM
  model uses no past information and thus only suggests common words starting
  with the letter "l".<br>

  The LSTM model suggests variations on the word "language",
  which makes sense given the text is about learning grammar. The GRU model
  does one better, by using the words "study" and "education" to also suggests
  the correct word "learning".
    </li>
    <li>
  The second observation, is when the models should predict the word "grammar".
  This word appears twice, for the first case very little is known
  about the context. Thus, the models don't suggest "grammar" until they have
  <a href="javascript:connectivitySetIndex(31);">seen 3</a> or
  <a href="javascript:connectivitySetIndex(32);">4 characters</a>.<br>

  For the second occurrence, there is much more text for the models to
  understand the context of the word. The GRU model is thus able to predict
  the word "grammar" with only <a href="javascript:connectivitySetIndex(159);">
  1 character from the word itself</a>. While the LSTM and Nested LSTM again
  needs <a href="javascript:connectivitySetIndex(162);">4 characters</a>.
    </li>
  </ul>
  <p>
    These observations suggest that the Nested LSTM model, in particular,
    doesn't use long-term memorization as one would otherwise expect from the
    additional memory states.
  </p>
</dt-article>

<dt-article>
  <h2>Conclusion</h2>
  <p>
    Looking at accuracy and cross entropy loss in itself is not that
    interesting. A reasonable high accuracy can be obtained, with reasonably
    low contextual understanding. It is only for the first couple of characters,
    that long-term memorization and contextual understanding really matters.
    More traditional problems like Penn Treebank, Chinese Poetry Generation, or
    text8 generation are therefore just as suited for this kind of analysis.
  </p>
  <p>
    The more qualitative analysis of the connectivity together with the
    autocomplete predictions, reveals that the Nested LSTM model isn't capable
    of long-term contextual understanding. This contradicts the design decisions
    for the model and the findings in the original paper
    <dt-cite key="moniz2018nlstm"></dt-cite>. In fact, the GRU model, which has
    no internal memory, is the model that has the highest accuracy and shows
    the most contextual understanding in the connectivity analysis.
  </p>
  <p>
    It is entirely possible, that the results are highly dependent on the
    random initialization and the specific application. However, the results
    still show that more internal memory per recurrent unit does not necessarily
    improve long-term memorization as hypothesized in the Nested LSTM paper
    <dt-cite key="moniz2018nlstm"></dt-cite>.
  </p>
</dt-article>

<dt-appendix>
  <h3>Acknowledgments</h3>
  <p>
    Many thanks to the authors of the original Nested LSTM paper
    <dt-cite key="moniz2018nlstm"></dt-cite>, Joel Ruben, Antony Moniz,
    and David Krueger. Even though our findings weren't the same, they
    have inspired much of this article and shown that something as used
    as the recurrent unit is still an open research area.
  </p>
  <p>
    I'm also grateful for the excellent feedback and patience by
    Christopher Olah from the Distill team. His feedback has dramatically
    improved the quality of the article.
  </p>
  <h3 id="appendix-lstm">Long Short-Term Memory</h3>
  <figure>
    <img style="width: 100%;" src="graphics/lstm-feedback-1-web.svg" alt="Long Short Term Memory Diagram">
    <figcaption>
      <strong>LSTM:</strong> (Long Short-Term Memory) allows for long-term
      memorization by gateing its update, thereby solving the vanishing gradient
      problem.
    </figcaption>
  </figure>
  <p>
  The equations defining <math-latex latex="
  \mathrm{LSTM}(\cdot, \cdot)"></math-latex> as used in <math-latex latex="
  \mathrm{NLSTM}(\cdot, \cdot)"></math-latex> are:
  <math-latex display-mode latex="\begin{aligned}
    \tilde{i}_\ell^t &= \sigma_i(IW_{\ell-1, \ell} \tilde{h}_{\ell-1}^{t} + IW_{\ell, \ell} \tilde{h}_{\ell}^{t-1}) \\
    \tilde{f}_\ell^t &= \sigma_f(FW_{\ell-1, \ell} \tilde{h}_{\ell-1}^{t} + FW_{\ell, \ell} \tilde{h}_{\ell}^{t-1}) \\
    \tilde{o}_\ell^t &= \sigma_o(OW_{\ell-1, \ell} \tilde{h}_{\ell-1}^{t} + OW_{\ell, \ell} \tilde{h}_{\ell}^{t-1}) \\
    \tilde{z}_\ell^t &= \sigma_z(ZW_{\ell-1, \ell} \tilde{h}_{\ell-1}^{t} + ZW_{\ell, \ell} \tilde{h}_{\ell}^{t-1}) \\
    \tilde{c}_\ell^t &= \tilde{i}_\ell^t \odot \tilde{z}_\ell^t + \tilde{f}_\ell^t \odot \tilde{c}_\ell^{t-1} \\
    \tilde{h}_{\ell}^{t} &= \mathrm{LSTM}(\tilde{h}_{\ell-1}^{t}, \tilde{h}_{\ell}^{t-1}) \coloneqq \tilde{o}_\ell^t \odot \sigma_h(\tilde{c}_\ell^t)
  \end{aligned}"></math-latex>
  In terms of the Nested LSTM unit, <math-latex latex="
  \tilde{h}_{\ell-1}^{t} = i_\ell^t \odot z_\ell^t"></math-latex> and
  <math-latex latex="\tilde{h}_{\ell}^{t-1} = f_\ell^t \odot c_\ell^{t-1}"></math-latex>.
  </p>
  <p>
    The gate activation functions <math-latex latex="
    \left(\sigma_i(\cdot), \sigma_f(\cdot), \text{ and } \sigma_o(\cdot)\right)
    "></math-latex> are usually the simoid activation function.
    While <math-latex latex="
    \left(\sigma_z(\cdot) \text{ and } \sigma_h(\cdot)\right)
    "></math-latex> are usually <math-latex latex="\tanh(\cdot)"></math-latex>.
  </p>
</dt-appendix>

<script type="text/bibliography">
  @article{moniz2018nlstm,
    title={Nested LSTMs},
    author={Joel Ruben Antony Moniz and David Krueger},
    journal={arXivreprint arXiv:1801.10308},
    year={2018},
    url={https://arxiv.org/pdf/1801.10308.pdf}
  }

  @article{karpathy2015rnnvis,
    title={Visualizing and Understanding Recurrent Networks},
    author={Andrej Karpathy and Justin Johnson and Li Fei-Fei},
    journal={arXivreprint arXiv:1506.02078},
    year={2015},
    url={https://arxiv.org/pdf/1506.02078.pdf}
  }

  @article{graves2008phd,
    title={Supervised Sequence Labelling with Recurrent Neural Networks},
    author={Alex Graves},
    year={2008},
    url={https://www.cs.toronto.edu/~graves/phd.pdf}
  }

  @article{cho2014gru,
    title={Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation},
    author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
    journal={arXivreprint arXiv:1406.1078},
    year={2014},
    url={https://arxiv.org/pdf/1406.1078.pdf}
  }
</script>
